"""
ìƒì„¸ íƒì§€ ë¦¬í¬íŠ¸: ê° íƒì§€ëœ ì‚¬ê¸° ì§€ê°‘ì— ëŒ€í•œ ìƒì„¸ ì •ë³´
"""
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent / "src"))

import json
import pandas as pd
import numpy as np
from graph import CryptoTransactionGraph
from ppr import PersonalizedPageRank
from scoring import NormalizedScorer
from anomaly_detector import MPOCryptoMLDetector
from pattern_analyzer import PatternAnalyzer
from tqdm import tqdm
from datetime import datetime, timedelta
from sklearn.metrics import average_precision_score, roc_auc_score


def create_comprehensive_report():
    print("="*70)
    print("ìƒì„¸ íƒì§€ ë¦¬í¬íŠ¸ ìƒì„±")
    print("="*70)
    
    # 1. ê·¸ë˜í”„ ë¡œë“œ
    print("\n[Step 1] Loading graph...")
    filepath = "results/graph_200_etherscan_real.json"
    
    graph_obj = CryptoTransactionGraph()
    with open(filepath, 'r') as f:
        data = json.load(f)
        graph_obj.nodes = data['nodes']
        graph_obj.edges = [(e['from'], e['to'], e['value'], e['timestamp']) for e in data['edges']]
        graph_obj.node_labels = {k: int(v) for k, v in data['labels'].items()}
    
    # ì‹¤ì œ í•™ìŠµ/í‰ê°€ì— ì‚¬ìš©ë˜ëŠ” ì‚¬ê¸° ì§€ê°‘ ê°œìˆ˜
    actual_anomalies = sum([1 for n in graph_obj.nodes if n in graph_obj.node_labels and graph_obj.node_labels[n] == 1])
    
    print(f"  Nodes: {len(graph_obj.nodes)}")
    print(f"  Edges: {len(graph_obj.edges)}")
    print(f"  Anomalies (ì‹¤ì œ ê·¸ë˜í”„ì— í¬í•¨): {actual_anomalies}ê°œ")
    print(f"  ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ ì´ëŸ‰: {sum(graph_obj.node_labels.values())}ê°œ (200ê°œ ì¤‘ {sum(graph_obj.node_labels.values())-actual_anomalies}ê°œëŠ” ê·¸ë˜í”„ ì™¸ë¶€)")
    
    # 2. PPR ê³„ì‚°
    print("\n[Step 2] Computing PPR...")
    graph = graph_obj.build_graph()
    ppr = PersonalizedPageRank(graph, alpha=0.5, epsilon=0.01, p_f=1.0)
    
    source_nodes = ppr.get_source_nodes()
    if len(source_nodes) == 0:
        source_nodes = set(graph_obj.nodes[:20])
    
    print(f"  Found {len(source_nodes)} source nodes (in-degree=0)")
    
    # âš ï¸ ì„±ëŠ¥ ë¬¸ì œë¡œ ìƒ˜í”Œë§ (í•„ìš”ì‹œ ì „ì²´ ì‚¬ìš© ê°€ëŠ¥)
    # ì „ì²´ ì‚¬ìš©: sample_nodes = list(source_nodes)
    # ì‹¤í—˜: 50 â†’ 100ìœ¼ë¡œ ì¦ê°€
    sample_nodes = list(source_nodes)[:min(100, len(source_nodes))]  # 50 â†’ 100ìœ¼ë¡œ ì¦ê°€
    print(f"  Sampling {len(sample_nodes)} source nodes for PPR computation")
    
    ppr_results = {}
    
    print(f"  Computing PPR for {len(sample_nodes)} source nodes...")
    
    # Fix 3: ì „ì—­ pi aggregation (ì¸ë±ìŠ¤ ë¶ˆì¼ì¹˜ ë°©ì§€)
    ppr_by_source = {}
    for node in tqdm(sample_nodes, desc="  PPR", ncols=70):
        sps, svn, all_nodes_list = ppr.compute_single_source_ppr(node)
        ppr_results[node] = svn
        # dict í˜•íƒœë¡œ ì €ì¥ (node -> score)
        ppr_by_source[node] = {n: float(sps[i]) for i, n in enumerate(all_nodes_list)}
    
    # ì „ì—­ pi í•©ì‚°
    global_pi = {}
    for d in ppr_by_source.values():
        for n, val in d.items():
            global_pi[n] = global_pi.get(n, 0.0) + val
    
    # ì •ê·œí™” (ì„ íƒ)
    if global_pi:
        m = max(global_pi.values())
        if m > 0:
            for n in list(global_pi.keys()):
                global_pi[n] /= m
    
    # detectorëŠ” íŠœí”Œ í˜•íƒœ í•„ìš” (í˜¸í™˜ì„±)
    ppr_scores_dict = {}
    for source_node in sample_nodes:
        # ì „ì—­ pië¥¼ í™œìš©í•˜ë˜, detector í˜¸í™˜ì„± ìœ„í•´ ìœ ì§€
        sps_array = np.array([global_pi.get(n, 0.0) for n in ppr_results[source_node]])
        ppr_scores_dict[source_node] = (sps_array, list(ppr_results[source_node]))
    
    # 3. Features
    print("\n[Step 3] Computing features...")
    scorer = NormalizedScorer(graph_obj, ppr_results)
    feature_scores = scorer.compute_all_scores()
    
    # 4. Anomaly Detection
    print("\n[Step 4] Training model...")
    full_ppr_scores = {}
    for source in ppr_scores_dict.keys():
        full_ppr_scores[source] = ppr_scores_dict[source]
    
    detector = MPOCryptoMLDetector(
        ppr_scores=full_ppr_scores,
        feature_scores=feature_scores,
        labels=graph_obj.node_labels
    )
    
    detector.train_logistic_regression()
    detector.compute_pattern_scores()
    detector.compute_anomaly_scores()
    
    # 5. Pattern Analysis
    print("\n[Step 5] Analyzing patterns...")
    pattern_analyzer = PatternAnalyzer(graph_obj)
    
    # 6. ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
    print("\n" + "="*70)
    print("ìƒì„¸ íƒì§€ ë¦¬í¬íŠ¸")
    print("="*70)
    
    results_df = detector.get_results_df()
    results_df_sorted = results_df.sort_values('anomaly_score', ascending=False)
    
    # Top K ì´ìƒ ì§€ê°‘ ë¶„ì„
    k = 20
    top_k = results_df_sorted.head(k)
    
    # Fix 1: Top-K count (boolean -> int)
    detected_in_topk = int(top_k['label'].sum())
    
    print(f"\n[íƒì§€ í˜„í™©]")
    print(f"  ì „ì²´ ì‚¬ê¸° ì§€ê°‘: {sum(graph_obj.node_labels.values())}ê°œ (ë¼ë²¨ ë”•ì…”ë„ˆë¦¬)")
    print(f"  ê·¸ë˜í”„ì— í¬í•¨ëœ ì‚¬ê¸°: {actual_anomalies}ê°œ (ì‹¤ì œ í•™ìŠµ/í‰ê°€)")
    print(f"  ëª¨ë¸ íƒì§€ (Top {k}): {detected_in_topk}ê°œ")
    print(f"  Precision@{k}: {detected_in_topk / k:.4f}")
    
    # ì¶”ê°€ ì§€í‘œ
    y_true = top_k['label'].values.astype(int)
    y_score = top_k['anomaly_score'].values
    try:
        ap = average_precision_score(y_true, y_score)
        auc = roc_auc_score(y_true, y_score)
        print(f"  Average Precision: {ap:.4f}")
        print(f"  ROC-AUC: {auc:.4f}")
    except:
        pass
    
    print(f"\n[Anomaly Score ìƒìœ„ {k}ê°œ ì§€ê°‘ ìƒì„¸ ì •ë³´]")
    print("="*70)
    print("â€» ì´ ì§€ê°‘ë“¤ì€ ëª¨ë¸ì´ 'ê°€ì¥ ì´ìƒí•˜ë‹¤'ê³  íŒë‹¨í•œ ì§€ê°‘ë“¤ì…ë‹ˆë‹¤.")
    print("â€» ì •ìƒ ì§€ê°‘ì´ë”ë¼ë„ ëª¨ë¸ì´ ì´ìƒí•˜ê²Œ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("="*70)
    
    # PPR ì ìˆ˜ ê³„ì‚° (Ï€(vi))
    first_source = list(full_ppr_scores.keys())[0]
    ppr_array, all_nodes_list = full_ppr_scores[first_source]
    node_to_idx = {node: idx for idx, node in enumerate(all_nodes_list)}
    
    reports = []
    
    for idx, (_, row) in enumerate(top_k.iterrows(), 1):
        node = row['node']
        label = row['label']
        sigma = row['anomaly_score']
        nts = row['nts']
        nws = row['nws']
        pattern_score = row['pattern_score']
        
        # Ï€(vi) ê³„ì‚° (Fix 3: global_pi ì‚¬ìš©)
        pi_sum = float(global_pi.get(node, 0.0))
        
        # Fix 2: íŒ¨í„´ ë¶„ì„ (ê¸°ë³¸ê°’ ë¨¼ì € ì„¤ì •)
        pattern_type = "Unknown"
        pattern_details = {"in_degree": 0, "out_degree": 0, "omega_in": 0.0, "omega_out": 0.0}
        detected_patterns = []
        
        try:
            pattern_type, pattern_details = pattern_analyzer.analyze_pattern(node)
            if pattern_type and pattern_type != "Balanced":
                detected_patterns.append(pattern_type)
            
            # Fix 5: êµ¬ì²´ì ì¸ ë¼ë²¨ ì‚¬ìš©
            if pattern_details.get('in_degree', 0) > pattern_details.get('out_degree', 0) * 3:
                detected_patterns.append("Rapid-layering")
            if pattern_details.get('omega_in', 0) > pattern_details.get('omega_out', 0) * 2:
                detected_patterns.append("Value-mismatch")
        except Exception as e:
            pass
        
        # Evidence summary ìƒì„±
        in_count = pattern_details.get('in_degree', 0)
        out_count = pattern_details.get('out_degree', 0)
        in_amount = pattern_details.get('omega_in', 0)
        
        # Fix 6: ë¬¸ì¥ í†¤ ê°œì„ 
        if label == 1:
            evidence = f"ğŸ”´ ì‹¤ì œ ì‚¬ê¸° ì§€ê°‘. {in_count}íšŒ ì…ê¸ˆ, {out_count}íšŒ ì†¡ê¸ˆ. ì´ ì…ê¸ˆ: {in_amount:.2f} ETH. íŒ¨í„´: {', '.join(detected_patterns) if detected_patterns else 'Unknown'}"
        else:
            evidence = f"ğŸŸ¢ ë¹„ì‚¬ê¸° ì¶”ì •(ëª¨ë¸ ê¸°ì¤€). {in_count}íšŒ ì…ê¸ˆ, {out_count}íšŒ ì†¡ê¸ˆ. ì´ ì…ê¸ˆ: {in_amount:.2f} ETH. NTS={nts:.2f}, NWS={nws:.2f}"
        
        # Fix 8: Top paths (incoming + outgoing)
        top_paths = []
        try:
            # Incoming ê±°ë˜
            incoming = [(e['from'], e['to'], e['value']) for e in data['edges'] if e['to'] == node]
            incoming_sorted = sorted(incoming, key=lambda x: x[2], reverse=True)[:3]
            
            for from_addr, to_addr, value in incoming_sorted:
                top_paths.append(f"{from_addr[:20]}... â†’ {value:.2f} ETH")
            
            # Outgoing ê±°ë˜
            outgoing = [(e['from'], e['to'], e['value']) for e in data['edges'] if e['from'] == node]
            outgoing_sorted = sorted(outgoing, key=lambda x: x[2], reverse=True)[:3]
            
            for from_addr, to_addr, value in outgoing_sorted:
                top_paths.append(f"â†’ {to_addr[:20]}... : {value:.2f} ETH")
        except:
            pass
        
        print(f"\n[{idx}] {node[:50]}...")
        print(f"  ë¼ë²¨: {'ğŸ”´ ì‚¬ê¸°' if label == 1 else 'ğŸŸ¢ ì •ìƒ'}")
        print(f"  Ïƒ(vi): {sigma:.4f}")
        print(f"  Ï€(vi): {pi_sum:.6f}")
        print(f"  NTS: {nts:.4f}")
        print(f"  NWS: {nws:.4f}")
        print(f"  íŒ¨í„´: {', '.join(detected_patterns) if detected_patterns else 'None'}")
        print(f"  ê·¼ê±°: {evidence}")
        if top_paths:
            print(f"  ê²½ë¡œ: {top_paths[0]}")
        
        reports.append({
            'rank': idx,
            'address': node,
            'label': label,
            'sigma': sigma,
            'pi': pi_sum,
            'pattern_score': pattern_score,
            'NTS': nts,
            'NWS': nws,
            'detected_patterns': detected_patterns,
            'evidence_summary': evidence,
            'top_paths': top_paths[:3]
        })
    
    print("\n" + "="*70)
    print("âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
    print("="*70)
    
    # Fix 4: ë””ë ‰í† ë¦¬ ìƒì„±
    Path("results").mkdir(parents=True, exist_ok=True)
    
    # ë¦¬í¬íŠ¸ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    report_path = "results/detection_report.json"
    with open(report_path, 'w') as f:
        json.dump(reports, f, indent=2)
    
    print(f"\nğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    
    return reports


if __name__ == "__main__":
    reports = create_comprehensive_report()

